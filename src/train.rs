//! # å­¦ç¿’ãƒ­ã‚¸ãƒƒã‚¯
//!
//! `burn`ã®`Learner` APIã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹ã‚’ç®¡ç†ã—ã¾ã™ã€‚

use crate::model::TuningForkPINN;
use crate::physics::tuning_fork_loss;
use burn::{
    backend::{
        Autodiff,
        ndarray::{NdArray, NdArrayDevice},
    },
    config::Config,
    data::{dataloader::DataLoaderBuilder, dataloader::batcher::Batcher, dataset::Dataset},
    lr_scheduler::constant::ConstantLr,
    module::Module,
    optim::AdamConfig,
    prelude::*,
    record::{CompactRecorder, Recorder},
    tensor::Distribution,
    tensor::backend::AutodiffBackend,
    train::{LearnerBuilder, RegressionOutput, TrainOutput, TrainStep, ValidStep},
};

/// å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’ã‚ªãƒ³ã‚¶ãƒ•ãƒ©ã‚¤ã§ç”Ÿæˆã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€‚
/// ç‰©ç†ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§ã‚ã‚‹ãŸã‚ã€äº‹å‰ã«ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”¨æ„ã™ã‚‹å¿…è¦ãŒãªãã€
/// å¿…è¦ã«ãªã‚‹ãŸã³ã«ãƒ©ãƒ³ãƒ€ãƒ ãªå‘¨æ³¢æ•°ã‚’ç”Ÿæˆã—ã¾ã™ã€‚
#[derive(Clone, Debug)]
pub struct TuningForkDataset {
    /// ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®è¦‹ã‹ã‘ä¸Šã®ã‚µã‚¤ã‚ºã€‚
    pub size: usize,
    /// ç”Ÿæˆã™ã‚‹å‘¨æ³¢æ•°ã®ç¯„å›² (min, max)ã€‚
    pub freq_range: (f32, f32),
}

impl Dataset<f32> for TuningForkDataset {
    /// ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰ä¸€ã¤ã®ã‚¢ã‚¤ãƒ†ãƒ ï¼ˆå‘¨æ³¢æ•°ï¼‰ã‚’å–å¾—ã—ã¾ã™ã€‚
    /// ã“ã®å®Ÿè£…ã§ã¯ã€å‘¼ã°ã‚Œã‚‹ãŸã³ã«æ–°ã—ã„ãƒ©ãƒ³ãƒ€ãƒ ãªå‘¨æ³¢æ•°ã‚’è¿”ã—ã¾ã™ã€‚
    fn get(&self, _index: usize) -> Option<f32> {
        let frequency = Tensor::<NdArray<f32>, 1>::random(
            [1],
            // ä¿®æ­£: Distribution::Uniformã¯f64ã‚’è¦æ±‚ã™ã‚‹ãŸã‚ã€f32ã‹ã‚‰å¤‰æ›ã™ã‚‹
            Distribution::Uniform(self.freq_range.0.into(), self.freq_range.1.into()),
            &Default::default(),
        )
        .into_data()
        .into_vec::<f32>()
        .unwrap()[0];
        Some(frequency)
    }
    fn len(&self) -> usize {
        self.size
    }
}

/// ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰å–å¾—ã—ãŸã‚¢ã‚¤ãƒ†ãƒ ã‚’ãƒŸãƒ‹ãƒãƒƒãƒã«ã¾ã¨ã‚ã‚‹ãƒãƒƒãƒãƒ£ã€‚
pub struct TuningForkBatcher<B: Backend> {
    _device: B::Device,
}

impl<B: Backend> TuningForkBatcher<B> {
    pub fn new(device: B::Device) -> Self {
        Self { _device: device }
    }
}

impl<B: Backend> Batcher<B, f32, Tensor<B, 2>> for TuningForkBatcher<B> {
    /// `f32`ã®Vecã‚’`[batch_size, 1]`å½¢çŠ¶ã®ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›ã—ã¾ã™ã€‚
    fn batch(&self, items: Vec<f32>, device: &B::Device) -> Tensor<B, 2> {
        let tensors: Vec<_> = items
            .iter()
            .map(|item| Tensor::<B, 1>::from_floats([*item], device))
            .collect();
        Tensor::cat(tensors, 0).reshape([-1, 1])
    }
}

/// ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—ã‚’å®šç¾©ã—ã¾ã™ã€‚
impl<B: AutodiffBackend> TrainStep<Tensor<B, 2>, RegressionOutput<B>> for TuningForkPINN<B> {
    fn step(&self, item: Tensor<B, 2>) -> TrainOutput<RegressionOutput<B>> {
        let predicted_dims = self.forward(item.clone());
        let loss = tuning_fork_loss(predicted_dims.clone(), item.clone());
        let output = RegressionOutput {
            loss: loss.clone(),
            output: predicted_dims,
            targets: item,
        };
        TrainOutput::new(self, loss.backward(), output)
    }
}

/// ãƒ¢ãƒ‡ãƒ«ã®æ¤œè¨¼ã‚¹ãƒ†ãƒƒãƒ—ã‚’å®šç¾©ã—ã¾ã™ã€‚
impl<B: Backend> ValidStep<Tensor<B, 2>, RegressionOutput<B>> for TuningForkPINN<B> {
    fn step(&self, item: Tensor<B, 2>) -> RegressionOutput<B> {
        let predicted_dims = self.forward(item.clone());
        let loss = tuning_fork_loss(predicted_dims.clone(), item.clone());
        RegressionOutput {
            loss,
            output: predicted_dims,
            targets: item,
        }
    }
}

/// å­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹å…¨ä½“ã®è¨­å®šã‚’ä¿æŒã—ã¾ã™ã€‚
#[derive(Config)]
pub struct TrainingConfig {
    pub optimizer: AdamConfig,
    #[config(default = 1e-4)]
    pub learning_rate: f64,
    #[config(default = 10000)]
    pub num_epochs: usize,
    #[config(default = 128)]
    pub batch_size: usize,
}

/// å­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
pub fn run() {
    // ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã®å‹ã‚’ndarrayã«å›ºå®š
    type AppBackend = NdArray<f32>;
    type AppAutodiffBackend = Autodiff<AppBackend>;

    let device: NdArrayDevice = Default::default();
    let config = TrainingConfig::new(AdamConfig::new());
    let artifact_dir = "./artifacts";

    // å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
    let batcher_train = TuningForkBatcher::<AppAutodiffBackend>::new(device.clone());
    let dataloader_train = DataLoaderBuilder::new(batcher_train)
        .batch_size(config.batch_size)
        .num_workers(4)
        .build(TuningForkDataset {
            size: config.batch_size * 100,
            freq_range: (200.0, 1800.0), // å­¦ç¿’ç”¨ã®å‘¨æ³¢æ•°ç¯„å›²
        });

    // æ¤œè¨¼ç”¨ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
    let batcher_valid = TuningForkBatcher::<AppBackend>::new(device.clone());
    let dataloader_valid = DataLoaderBuilder::new(batcher_valid)
        .batch_size(config.batch_size)
        .num_workers(4)
        .build(TuningForkDataset {
            size: config.batch_size * 20,
            freq_range: (1800.0, 2000.0), // æ¤œè¨¼ç”¨ã®å‘¨æ³¢æ•°ç¯„å›²
        });

    let scheduler = ConstantLr::new(config.learning_rate);

    // Learnerã‚’æ§‹ç¯‰
    let learner = LearnerBuilder::new(artifact_dir)
        .devices(vec![device.clone()])
        .num_epochs(config.num_epochs)
        .build(
            TuningForkPINN::<AppAutodiffBackend>::new(&device),
            config.optimizer.init(),
            scheduler,
        );

    println!("ğŸš€ Starting training on {:?}...", device);
    let model_trained = learner.fit(dataloader_train, dataloader_valid);

    // å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜
    let model_record = model_trained.into_record();
    CompactRecorder::new()
        .record(model_record, format!("{artifact_dir}/model").into())
        .expect("Failed to save trained model");

    println!("\nâœ… Model saved to '{artifact_dir}/model.mpk'");
}
