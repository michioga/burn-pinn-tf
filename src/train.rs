//! # å­¦ç¿’ãƒ­ã‚¸ãƒƒã‚¯
//!
//! `burn`ã®`Learner` APIã‚’ä½¿ç”¨ã—ã¦ã€ç‰©ç†æƒ…å ±ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆPINNï¼‰ã®å­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹ã‚’ç®¡ç†ã—ã¾ã™ã€‚
//! ã“ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¯ã€ä»»æ„ã®ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã§å‹•ä½œã™ã‚‹ã‚ˆã†ã«ã‚¸ã‚§ãƒãƒªãƒƒã‚¯ã«ãªã£ã¦ã„ã¾ã™ã€‚

use crate::model::TuningForkPINN;
use crate::physics::tuning_fork_loss;
use burn::{
    config::Config,
    data::{dataloader::DataLoaderBuilder, dataloader::batcher::Batcher, dataset::Dataset},
    lr_scheduler::constant::ConstantLr,
    module::Module,
    optim::AdamConfig,
    prelude::*,
    record::{CompactRecorder, Recorder},
    tensor::backend::AutodiffBackend,
    train::{LearnerBuilder, RegressionOutput, TrainOutput, TrainStep, ValidStep},
};
use rand::{Rng, thread_rng};

/// å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’ã‚ªãƒ³ã‚¶ãƒ•ãƒ©ã‚¤ã§ç”Ÿæˆã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€‚
///
/// ç‰©ç†ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§ã‚ã‚‹ãŸã‚ã€äº‹å‰ã«ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”¨æ„ã™ã‚‹å¿…è¦ãŒãªãã€
/// å¿…è¦ã«ãªã‚‹ãŸã³ã«ãƒ©ãƒ³ãƒ€ãƒ ãªå‘¨æ³¢æ•°ã‚’ç”Ÿæˆã—ã¾ã™ã€‚
#[derive(Clone, Debug)]
pub struct TuningForkDataset {
    /// ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®è¦‹ã‹ã‘ä¸Šã®ã‚µã‚¤ã‚ºã€‚
    pub size: usize,
    /// ç”Ÿæˆã™ã‚‹å‘¨æ³¢æ•°ã®ç¯„å›² (min, max)ã€‚
    pub freq_range: (f32, f32),
}

impl Dataset<f32> for TuningForkDataset {
    /// ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰ä¸€ã¤ã®ã‚¢ã‚¤ãƒ†ãƒ ï¼ˆå‘¨æ³¢æ•°ï¼‰ã‚’å–å¾—ã—ã¾ã™ã€‚
    ///
    /// ã“ã®å®Ÿè£…ã§ã¯ã€å‘¼ã°ã‚Œã‚‹ãŸã³ã«æ–°ã—ã„ãƒ©ãƒ³ãƒ€ãƒ ãªå‘¨æ³¢æ•°ã‚’è¿”ã—ã¾ã™ã€‚
    fn get(&self, _index: usize) -> Option<f32> {
        let mut rng = thread_rng();
        let frequency = rng.gen_range(self.freq_range.0..=self.freq_range.1);
        Some(frequency)
    }

    /// ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®é•·ã•ã‚’è¿”ã—ã¾ã™ã€‚
    fn len(&self) -> usize {
        self.size
    }
}

/// ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰å–å¾—ã—ãŸã‚¢ã‚¤ãƒ†ãƒ ã‚’ãƒŸãƒ‹ãƒãƒƒãƒã«ã¾ã¨ã‚ã‚‹ãƒãƒƒãƒãƒ£ã€‚
///
/// `f32`ã®ã‚¹ãƒ©ã‚¤ã‚¹ã‚’ã€æŒ‡å®šã•ã‚ŒãŸãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã®ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›ã—ã¾ã™ã€‚
pub struct TuningForkBatcher<B: Backend> {
    _device: B::Device,
}

impl<B: Backend> TuningForkBatcher<B> {
    /// æ–°ã—ã„ãƒãƒƒãƒãƒ£ã‚’ä½œæˆã—ã¾ã™ã€‚
    pub fn new(device: B::Device) -> Self {
        Self { _device: device }
    }
}

impl<B: Backend> Batcher<B, f32, Tensor<B, 2>> for TuningForkBatcher<B> {
    /// `f32`ã®Vecã‚’`[batch_size, 1]`å½¢çŠ¶ã®ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›ã—ã¾ã™ã€‚
    fn batch(&self, items: Vec<f32>, device: &B::Device) -> Tensor<B, 2> {
        let tensors: Vec<_> = items
            .iter()
            .map(|item| Tensor::<B, 1>::from_floats([*item], device))
            .collect();
        Tensor::cat(tensors, 0).reshape([-1, 1])
    }
}

/// ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—ã‚’å®šç¾©ã—ã¾ã™ã€‚
impl<B: AutodiffBackend> TrainStep<Tensor<B, 2>, RegressionOutput<B>> for TuningForkPINN<B> {
    /// 1å›ã®å­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
    ///
    /// 1. ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹äºˆæ¸¬
    /// 2. ç‰©ç†æ³•å‰‡ã«åŸºã¥ã„ãŸæå¤±ã®è¨ˆç®—
    /// 3. å‹¾é…ã®è¨ˆç®—ã¨é€†ä¼æ’­
    fn step(&self, item: Tensor<B, 2>) -> TrainOutput<RegressionOutput<B>> {
        let predicted_dims = self.forward(item.clone());
        let loss = tuning_fork_loss(predicted_dims.clone(), item.clone());
        let output = RegressionOutput {
            loss: loss.clone(),
            output: predicted_dims,
            targets: item,
        };
        TrainOutput::new(self, loss.backward(), output)
    }
}

/// ãƒ¢ãƒ‡ãƒ«ã®æ¤œè¨¼ã‚¹ãƒ†ãƒƒãƒ—ã‚’å®šç¾©ã—ã¾ã™ã€‚
impl<B: Backend> ValidStep<Tensor<B, 2>, RegressionOutput<B>> for TuningForkPINN<B> {
    /// 1å›ã®æ¤œè¨¼ã‚¹ãƒ†ãƒƒãƒ—ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
    ///
    /// æå¤±ã‚’è¨ˆç®—ã—ã€å­¦ç¿’ã®é€²æ—ã‚’ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã—ã¾ã™ã€‚
    fn step(&self, item: Tensor<B, 2>) -> RegressionOutput<B> {
        let predicted_dims = self.forward(item.clone());
        let loss = tuning_fork_loss(predicted_dims.clone(), item.clone());
        RegressionOutput {
            loss,
            output: predicted_dims,
            targets: item,
        }
    }
}

/// å­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹å…¨ä½“ã®è¨­å®šã‚’ä¿æŒã—ã¾ã™ã€‚
#[derive(Config)]
pub struct TrainingConfig {
    /// ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã®è¨­å®šã€‚
    pub optimizer: AdamConfig,
    /// å­¦ç¿’ç‡ã€‚
    #[config(default = 1e-4)]
    pub learning_rate: f64,
    /// å­¦ç¿’ã‚¨ãƒãƒƒã‚¯æ•°ã€‚
    #[config(default = 10000)]
    pub num_epochs: usize,
    /// ãƒãƒƒãƒã‚µã‚¤ã‚ºã€‚
    #[config(default = 1024)]
    pub batch_size: usize,
}

/// å­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
///
/// # Type Parameters
///
/// * `B` - å­¦ç¿’ã«ä½¿ç”¨ã™ã‚‹ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ï¼ˆä¾‹: `Autodiff<Wgpu>`ã€`Autodiff<NdArray>`ï¼‰ã€‚
///
/// # Arguments
///
/// * `device` - å­¦ç¿’ã«ä½¿ç”¨ã™ã‚‹ãƒ‡ãƒã‚¤ã‚¹ã€‚
pub fn run<B: AutodiffBackend>(device: B::Device)
where
    B::InnerBackend: Backend,
{
    let config = TrainingConfig::new(AdamConfig::new());
    let artifact_dir = "./artifacts";

    // å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
    let batcher_train = TuningForkBatcher::<B>::new(device.clone());
    let dataloader_train = DataLoaderBuilder::new(batcher_train)
        .batch_size(config.batch_size)
        .num_workers(4)
        .build(TuningForkDataset {
            size: config.batch_size * 100,
            freq_range: (200.0, 1800.0), // å­¦ç¿’ç”¨ã®å‘¨æ³¢æ•°ç¯„å›²
        });

    // æ¤œè¨¼ç”¨ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
    let batcher_valid = TuningForkBatcher::<B::InnerBackend>::new(device.clone());
    let dataloader_valid = DataLoaderBuilder::new(batcher_valid)
        .batch_size(config.batch_size)
        .num_workers(4)
        .build(TuningForkDataset {
            size: config.batch_size * 20,
            freq_range: (1800.0, 2000.0), // æ¤œè¨¼ç”¨ã®å‘¨æ³¢æ•°ç¯„å›²
        });

    let scheduler = ConstantLr::new(config.learning_rate);

    // Learnerã‚’æ§‹ç¯‰
    let learner = LearnerBuilder::new(artifact_dir)
        .devices(vec![device.clone()])
        .num_epochs(config.num_epochs)
        .build(
            TuningForkPINN::<B>::new(&device),
            config.optimizer.init(),
            scheduler,
        );

    println!("ğŸš€ Starting training on {:?}...", device);
    let model_trained = learner.fit(dataloader_train, dataloader_valid);

    // å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜
    let model_record = model_trained.into_record();
    CompactRecorder::new()
        .record(model_record, format!("{artifact_dir}/model").into())
        .expect("Failed to save trained model");

    println!("\nâœ… Model saved to '{artifact_dir}/model.mpk'");
}

